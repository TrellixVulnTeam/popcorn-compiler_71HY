#!/usr/bin/python3

import os
import sys
import argparse
import subprocess
import datetime

import perfscrape
import responsetimescrape
from htmconfig import ConfigureHTM
from cycconfig import ConfigureCycles

###############################################################################
# Initialization
###############################################################################

def parseArguments():
    desc = "Automagically tune placement of migration points/HTM " \
           "transactions in an application by observing perf output of HTM " \
           "counters.  NOTE: the build environment must be set up to accept " \
           "environment variable HTM_FLAGS to configure the various " \
           "thresholds and add additional arguments."

    parser = argparse.ArgumentParser(description=desc,
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    running = parser.add_argument_group("Running")
    running.add_argument("-binary", type=str, required=True,
        help="Name of binary generated by build command",
        dest="binary")
    running.add_argument("-build-cmd", type=str, required=True,
        help="Command to build the binary, we'll append the configuration",
        dest="buildCmd")
    running.add_argument("-run-cmd", type=str, required=True,
        help="Command to run the binary",
        dest="runCmd")
    running.add_argument("-perf", type=str, help="Which version of perf to use",
        default="~/perf/perf",
        dest="perf")
    running.add_argument("-htm-perf", type=str,
        help="Command to run the HTM profiling script",
        default="htm-perf.sh",
        dest="htmPerf")
    running.add_argument("-response-perf", type=str,
        help="Command to run the response time profiling script",
        default="test-response-time.py",
        dest="respPerf")
    running.add_argument("-clean-cmd", type=str,
        help="Command to clean the build",
        default="make clean",
        dest="cleanCmd")

    tuning = parser.add_argument_group("Tuning")
    tuning.add_argument("-type", choices=["htm", "cycles"],
        help="Type of tuning",
        default="htm",
        dest="tuneType")
    tuning.add_argument("-max-iter", type=int,
        help="Maximum number of iterations to explore configuration",
        default=50,
        dest="maxIters")
    tuning.add_argument("-target-time", type=float, required=True,
        help="Time to run uninstrumented version of the application (used for " \
             "determining when to stop)",
        dest="targetTime")
    tuning.add_argument("-stop-thresh", type=int,
        help="Percent slowdown compared to target time at which stop " \
             "configuration exploration",
        default=5,
        dest="slowdownThresh")
    tuning.add_argument("-fast-tune", action="store_true",
        help="Only run perf-stat once per configuration to gather profiling " \
             "information rather than the normal 3 times (reduces stability)",
        dest="fastTune")

    htmTuning = parser.add_argument_group("HTM Tuning")
    htmTuning.add_argument("-max-func-iter", type=int,
        help="Maximum number of iterations to tune a single function",
        default=10,
        dest="maxFuncIters")

    return parser.parse_args()

def sanityCheckArgs(args):
    args.perf = os.path.abspath(args.perf)
    args.htmPerf = os.path.abspath(args.htmPerf)
    args.respPerf = os.path.abspath(args.respPerf)

    assert os.path.exists(args.perf), \
           "Perf binary '{}' doesn't exist!".format(args.perf)
    assert os.path.exists(args.htmPerf), \
           "HTM profiling script '{}' doesn't exist!".format(args.htmPerf)
    assert os.path.exists(args.respPerf), \
           "Response time profiling script '{}' doesn't exist!" \
           .format(args.respPerf)
    assert args.maxIters > 0 and args.maxIters < 100, \
           "Invalid maximum number of iterations"
    assert args.maxFuncIters > 0 and args.maxFuncIters <= args.maxIters, \
           "Invalid maximum number of per-function iterations"
    assert args.targetTime > 0, "Invalid target runtime"
    assert args.slowdownThresh > 0, "Invalid slowdown percentage threshold"

def writeReadme(readme, args):
    readme.write("Results for '{}':\n".format(args.binary))
    readme.write("  Build command: '{}'\n".format(args.buildCmd))
    readme.write("  Run command: '{}'\n".format(args.runCmd))
    readme.write("  Perf binary: {}\n".format(args.perf))
    readme.write("  HTM profiling script: {}\n".format(args.htmPerf))
    readme.write("  Response time profiling script: {}\n".format(args.respPerf))
    readme.write("  Clean command: '{}'\n\n".format(args.cleanCmd))

    readme.write("Tuning configuration:\n");
    readme.write("  Type: {}\n".format(args.tuneType))
    readme.write("  Max iterations: {}\n".format(args.maxIters))
    if args.tuneType == "htm":
        readme.write("  Max iterations per function: {}\n" \
                     .format(args.maxFuncIters))
    readme.write("  Target time: {}\n".format(args.targetTime))
    readme.write("  Stop threshold: {}%\n".format(args.slowdownThresh))
    if(args.fastTune): readme.write("  + Fast tune\n")

def initialize(args):
    sanityCheckArgs(args)
    now = datetime.datetime.now()
    results = "{}-instrument-{}:{:0>2}-{}-{}-{}".format(
        args.tuneType, now.hour, now.minute, now.day, now.month, now.year)
    assert not os.path.exists(results), \
           "Tuning output folder '{}' already exists!".format(results)
    os.makedirs(results)
    with open(results + "/README", 'w') as readme: writeReadme(readme, args)
    return results

###############################################################################
# Generating & running the binary
###############################################################################

# Run the clean command to remove previous builds.
def cleanBuild(cleanCmd):
    try:
        args = cleanCmd.strip().split()
        rv = subprocess.check_call(args, stderr=subprocess.STDOUT)
    except Exception as e:
        print("ERROR: could not clean in preparation for building ({})!" \
              .format(e))
        sys.exit(1)
    else:
        if rv != 0:
            print("ERROR: clean command returned non-zero!")
            sys.exit(1)

# Run the build command to generate a binary with a given configuration.
def buildBinary(buildCmd, binary, cap, start, ret, other):
    try:
        args = buildCmd.strip().split()
        args.append("HTM_FLAGS=-mllvm -cap-threshold={} " \
                    "-mllvm -start-threshold={} " \
                    "-mllvm -ret-threshold={} " \
                    "{}".format(cap, start, ret, other))
        rv = subprocess.check_call(args, stderr=subprocess.STDOUT)
    except Exception as e:
        print("Could not build the binary:\n{}".format(e))
        sys.exit(1)
    else:
        if rv != 0:
            print("Build command returned non-zero!")
            sys.exit(1)

    assert os.path.isfile(binary), \
           "Binary '{}' does not exist after build!".format(binary)

def runBinary(outputFolder, runCmd, perf, htmPerf, respPerf, binary, fast):
    try:
        # Pure event counts without migration signaling
        args = [ htmPerf, "-p", perf, "--" ]
        if fast: args[3:3] = [ "-r", "1" ]
        args.extend(runCmd.strip().split())
        out = subprocess.check_output(args, stderr=subprocess.STDOUT)

        # Measure response times for migration signaling
        respDest = outputFolder + "/{}.resp.out" \
                                  .format(os.path.basename(binary))
        args = [ respPerf, "-run-cmd", runCmd, "-output", respDest ]
        subprocess.check_call(args, stderr=subprocess.STDOUT)
    except Exception as e:
        print("Could not run the binary ({})!".format(e))
        sys.exit(1)

    # Write output to file
    outfile = outputFolder + "/{}.out".format(os.path.basename(binary))
    with open(outfile, 'w') as fp: fp.write(out.decode("utf-8"))

    # The run command should have generated a counters file (*.log) and a
    # sampling file (*.data)
    statOutput = "{}.log".format(binary)
    recordOutput = "{}.data".format(binary)
    assert os.path.isfile(statOutput), \
           "perf-stat output {} does not exist after run!".format(statOutput)
    assert os.path.isfile(recordOutput), \
           "perf-record output {} does not exist after run!" \
           .format(recordOutput)
    statDest = outputFolder + '/' + os.path.basename(statOutput)
    recordDest = outputFolder + '/' + os.path.basename(recordOutput)
    os.rename(statOutput, statDest)
    os.rename(recordOutput, recordDest)
    return statDest, recordDest, respDest

def runConfiguration(args, iteration, results, cap, start, ret, other):
    # Make an output folder for the current iteration.
    iterDir = results + "/" + str(iteration)
    os.makedirs(iterDir)

    # Clean/build/run the current configuration
    cleanBuild(args.cleanCmd)
    buildBinary(args.buildCmd, args.binary, cap, start, ret, other)
    return runBinary(iterDir, args.runCmd, args.perf, args.htmPerf,
                     args.respPerf, args.binary, args.fastTune)

###############################################################################
# Driver
###############################################################################

if __name__ == "__main__":
    args = parseArguments()
    results = initialize(args)
    if args.tuneType == "htm":
      marcoPolo = ConfigureHTM(args.targetTime, args.slowdownThresh,
                               args.maxIters, args.maxFuncIters, results)
    else:
      marcoPolo = ConfigureCycles(args.targetTime, args.slowdownThresh,
                                  args.maxIters, results)

    while marcoPolo.keepGoing:
        cap, start, ret, other = marcoPolo.getConfiguration()
        stat, record, resp = runConfiguration(args, marcoPolo.iteration,
                                              results, cap, start, ret, other)
        time, counters = perfscrape.scrapePerfStat(stat)
        numSamples, eventCount, samples = \
            perfscrape.scrapePerfReport(args.perf, record)
        respStats, respTimes, migLibCalls = \
            responsetimescrape.scrapeResponseTimes(resp)
        marcoPolo.analyze(time, counters, numSamples, samples, respStats)

    marcoPolo.writeBest()

